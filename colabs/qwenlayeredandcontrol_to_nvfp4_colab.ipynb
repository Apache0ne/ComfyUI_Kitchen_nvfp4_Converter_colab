{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title üß∞ Pre-convert setup (installs + folders)\n",
        "import os, sys, platform, torch\n",
        "\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Platform:\", platform.platform())\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "    print(\"Compute capability:\", torch.cuda.get_device_capability(0))\n",
        "    print(\"CUDA runtime:\", torch.version.cuda)\n",
        "\n",
        "!pip -q install -U safetensors tqdm requests==2.32.4 huggingface_hub hf_transfer\n",
        "!pip -q install -U comfy-kitchen\n",
        "\n",
        "# optional: faster Hugging Face transfers\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "print(\"HF_HUB_ENABLE_HF_TRANSFER=1\")\n",
        "\n",
        "MODEL_DIR = \"/content/models\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "print(\"MODEL_DIR:\", MODEL_DIR)"
      ],
      "metadata": {
        "id": "HgOrhN4uVrYk"
      },
      "id": "HgOrhN4uVrYk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o11YactYiQyi"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title üì• Download BF16 model from Hugging Face\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "REPO_ID = \"Comfy-Org/Qwen-Image-Layered_ComfyUI\" #\n",
        "FILENAME = \"split_files/diffusion_models/qwen_image_layered_bf16.safetensors\" #\n",
        "\n",
        "input_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n",
        "print(\"Downloaded to:\", input_path)\n",
        "# After hf_hub_download(...)\n",
        "downloaded_path = input_path  # so key-check + convert cells that expect downloaded_path still work\n",
        "print(\"downloaded_path set to:\", downloaded_path)\n"
      ],
      "id": "o11YactYiQyi"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üì• Download from Civitai (modelId + modelVersionId) slower\n",
        "import os, re, requests\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ===== YOU SET THESE =====\n",
        "CIVITAI_MODEL_ID = \"\"          # optional (info lookup only)\n",
        "CIVITAI_VERSION_ID = \"\"        # REQUIRED\n",
        "CIVITAI_API_TOKEN = \"\"         # optional; needed for models requiring login\n",
        "FILENAME_OVERRIDE = \"\"         # optional\n",
        "# =========================\n",
        "\n",
        "def _filename_from_cd(cd: str):\n",
        "    if not cd:\n",
        "        return None\n",
        "    m = re.search(r'filename\\*?=(?:UTF-8\\'\\')?\"?([^\\\";]+)\"?', cd, flags=re.IGNORECASE)\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "def civitai_download(model_version_id: str, out_dir: str, token: str = \"\", filename_override: str = \"\") -> str:\n",
        "    url = f\"https://civitai.com/api/download/models/{model_version_id}\"\n",
        "    # token via query string is explicitly supported\n",
        "    if token:\n",
        "        url += f\"?token={token}\"\n",
        "\n",
        "    with requests.get(url, stream=True, allow_redirects=True, timeout=(10, 120)) as r:\n",
        "        r.raise_for_status()\n",
        "        total = int(r.headers.get(\"Content-Length\", \"0\") or \"0\")\n",
        "        cd = r.headers.get(\"Content-Disposition\", \"\")\n",
        "\n",
        "        fname = filename_override.strip() or _filename_from_cd(cd) or f\"civitai_{model_version_id}.safetensors\"\n",
        "        out_path = os.path.join(out_dir, fname)\n",
        "\n",
        "        pbar = tqdm(total=total if total > 0 else None, unit=\"B\", unit_scale=True, desc=f\"Downloading {fname}\")\n",
        "        with open(out_path, \"wb\") as f:\n",
        "            for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "                    pbar.update(len(chunk))\n",
        "        pbar.close()\n",
        "\n",
        "    return out_path\n",
        "\n",
        "assert CIVITAI_VERSION_ID.strip(), \"Set CIVITAI_VERSION_ID (modelVersionId).\"\n",
        "downloaded_path = civitai_download(CIVITAI_VERSION_ID.strip(), MODEL_DIR, CIVITAI_API_TOKEN.strip(), FILENAME_OVERRIDE.strip())\n",
        "print(\"‚úÖ Downloaded:\", downloaded_path)"
      ],
      "metadata": {
        "id": "nUZ3F6bZV9SB",
        "cellView": "form",
        "collapsed": true
      },
      "id": "nUZ3F6bZV9SB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üßæ Key check: list safetensors keys (prints + saves full list) + prefix/pattern summary Just for adding model support\n",
        "import os, re, json\n",
        "import safetensors\n",
        "\n",
        "# --------- SETTINGS ----------\n",
        "PRINT_FIRST_N_KEYS = 160          # print first N keys to the notebook output\n",
        "SAVE_KEYS_TXT = \"/content/model_keys.txt\"\n",
        "\n",
        "# Patterns to quickly spot ‚Äúfamily‚Äù structure + likely blacklist candidates\n",
        "PATTERNS = [\n",
        "    # Qwen-family / likely sensitive heads\n",
        "    \"img_in\", \"txt_in\", \"time_text_embed\", \"norm_out\", \"proj_out\",\n",
        "    \"img_mod\", \"txt_mod\", \"txt_mlp\",\n",
        "\n",
        "    # Common diffusion/transformer naming\n",
        "    \"final_layer\", \"adaLN_modulation\", \"cap_embedder\", \"x_embedder\",\n",
        "    \"noise_refiner\", \"context_refiner\", \"t_embedder\",\n",
        "    \"attn\", \"qkv\", \"to_out\", \"mlp\", \"proj\", \"norm\"\n",
        "]\n",
        "\n",
        "# Probing reads a few tensors (dtype/shape) ‚Äî keep small to avoid slowdowns on huge models\n",
        "PROBE_TENSORS = True\n",
        "PROBE_MAX = 12\n",
        "\n",
        "# --------- VALIDATE ----------\n",
        "assert \"downloaded_path\" in globals(), \"downloaded_path is not set (run the download cell first).\"\n",
        "assert downloaded_path.endswith(\".safetensors\"), f\"Not a .safetensors file: {downloaded_path}\"\n",
        "assert os.path.exists(downloaded_path), f\"File not found: {downloaded_path}\"\n",
        "\n",
        "file_mb = os.path.getsize(downloaded_path) / (1024**2)\n",
        "print(\"‚úÖ File:\", downloaded_path)\n",
        "print(f\"‚úÖ Size: {file_mb:,.2f} MB\")\n",
        "\n",
        "# --------- READ KEYS + METADATA ----------\n",
        "with safetensors.safe_open(downloaded_path, framework=\"pt\") as f:\n",
        "    keys = list(f.keys())\n",
        "    meta = f.metadata() or {}\n",
        "\n",
        "print(\"\\n‚úÖ Key count:\", len(keys))\n",
        "print(f\"‚úÖ First {min(PRINT_FIRST_N_KEYS, len(keys))} keys:\")\n",
        "for k in keys[:PRINT_FIRST_N_KEYS]:\n",
        "    print(k)\n",
        "\n",
        "# Save all keys to text\n",
        "with open(SAVE_KEYS_TXT, \"w\", encoding=\"utf-8\") as w:\n",
        "    for k in keys:\n",
        "        w.write(k + \"\\n\")\n",
        "print(\"\\nüíæ Saved full key list to:\", SAVE_KEYS_TXT)\n",
        "\n",
        "# --------- METADATA QUICK VIEW ----------\n",
        "print(\"\\n‚úÖ Metadata keys (up to 60):\", list(meta.keys())[:60])\n",
        "# If quant metadata exists, print only its top-level key names (avoid dumping huge JSON)\n",
        "if \"_quantization_metadata\" in meta:\n",
        "    try:\n",
        "        qm = json.loads(meta[\"_quantization_metadata\"])\n",
        "        print(\"‚úÖ _quantization_metadata keys:\", list(qm.keys()))\n",
        "    except Exception as e:\n",
        "        print(\"‚ö†Ô∏è Could not parse _quantization_metadata JSON:\", str(e)[:200])\n",
        "\n",
        "# --------- PREFIX / STRUCTURE SUMMARY ----------\n",
        "has_cu_prefix = any(\"model.diffusion_model.\" in k for k in keys)\n",
        "print(\"\\n‚úÖ Has 'model.diffusion_model.' prefix?\", has_cu_prefix)\n",
        "\n",
        "def top_prefix_counts(keys_list, depth=1, topk=30):\n",
        "    counts = {}\n",
        "    for k in keys_list:\n",
        "        parts = k.split(\".\")\n",
        "        prefix = \".\".join(parts[:depth]) if len(parts) >= depth else k\n",
        "        counts[prefix] = counts.get(prefix, 0) + 1\n",
        "    items = sorted(counts.items(), key=lambda x: (-x[1], x[0]))[:topk]\n",
        "    return items\n",
        "\n",
        "print(\"\\nüì¶ Top prefixes (depth=1):\")\n",
        "for p, c in top_prefix_counts(keys, depth=1, topk=30):\n",
        "    print(f\"  {p:32s}  {c}\")\n",
        "\n",
        "print(\"\\nüì¶ Top prefixes (depth=2):\")\n",
        "for p, c in top_prefix_counts(keys, depth=2, topk=30):\n",
        "    print(f\"  {p:32s}  {c}\")\n",
        "\n",
        "# --------- PATTERN HITS (for blacklist/profile building) ----------\n",
        "def find_hits(substring):\n",
        "    return [k for k in keys if substring in k]\n",
        "\n",
        "print(\"\\nüîé Pattern hits summary:\")\n",
        "hits_summary = {}\n",
        "for pat in PATTERNS:\n",
        "    hits = find_hits(pat)\n",
        "    hits_summary[pat] = len(hits)\n",
        "    if hits:\n",
        "        print(f\"\\n‚Äî '{pat}' : {len(hits)} hit(s). Showing up to 25:\")\n",
        "        for h in hits[:25]:\n",
        "            print(\"  \", h)\n",
        "\n",
        "# Save pattern hits to file for easy copy/paste\n",
        "HITS_TXT = \"/content/key_hits.txt\"\n",
        "with open(HITS_TXT, \"w\", encoding=\"utf-8\") as w:\n",
        "    for pat in PATTERNS:\n",
        "        w.write(f\"[{pat}] ({hits_summary[pat]} hits)\\n\")\n",
        "        for h in find_hits(pat):\n",
        "            w.write(h + \"\\n\")\n",
        "        w.write(\"\\n\")\n",
        "print(\"\\nüíæ Saved pattern hits to:\", HITS_TXT)\n",
        "\n",
        "# --------- OPTIONAL: PROBE A FEW TENSORS (dtype/shape) ----------\n",
        "if PROBE_TENSORS:\n",
        "    probe_keys = []\n",
        "    for pat in PATTERNS:\n",
        "        hits = find_hits(pat)\n",
        "        if hits:\n",
        "            probe_keys.append(hits[0])\n",
        "        if len(probe_keys) >= PROBE_MAX:\n",
        "            break\n",
        "\n",
        "    if probe_keys:\n",
        "        print(\"\\nüß™ Probing a few tensors (dtype/shape):\")\n",
        "        with safetensors.safe_open(downloaded_path, framework=\"pt\") as f:\n",
        "            for k in probe_keys:\n",
        "                try:\n",
        "                    t = f.get_tensor(k)\n",
        "                    print(f\"  {k}\\n    dtype={t.dtype}  shape={tuple(t.shape)}  ndim={t.ndim}\")\n",
        "                except Exception as e:\n",
        "                    print(f\"  {k}\\n    ‚ö†Ô∏è probe failed: {str(e)[:200]}\")\n",
        "    else:\n",
        "        print(\"\\nüß™ No probe keys found (no PATTERNS matched).\")\n",
        "\n",
        "print(\"\\n‚úÖ Done.\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QHpHDLi7WEZE",
        "cellView": "form"
      },
      "id": "QHpHDLi7WEZE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_n4SbiQiQyi"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Write (convert_nvfp4.py)\n",
        "# =========================\n",
        "# Cell: Write converter module to disk (convert_nvfp4.py)\n",
        "# =========================\n",
        "%%writefile convert_nvfp4.py\n",
        "from __future__ import annotations\n",
        "\n",
        "import argparse\n",
        "import json\n",
        "import math\n",
        "import os\n",
        "import re\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, Tuple, List, Optional\n",
        "\n",
        "import torch\n",
        "from safetensors.torch import load_file, save_file\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Key routing (Qwen-layered + generic)\n",
        "# ============================================================\n",
        "\n",
        "# Qwen-ish patterns seen across Qwen/Qwen2/Qwen2.5 style checkpoints:\n",
        "# - model.layers.N.self_attn.(q_proj|k_proj|v_proj|o_proj).weight\n",
        "# - model.layers.N.mlp.(gate_proj|up_proj|down_proj).weight\n",
        "# Some repos use transformer.h.N.* instead of model.layers.N.*\n",
        "QWEN_ALLOWLIST_RE: List[re.Pattern] = [\n",
        "    re.compile(r\"^(model\\.layers\\.\\d+\\.self_attn\\.(q_proj|k_proj|v_proj|o_proj)\\.weight)$\"),\n",
        "    re.compile(r\"^(model\\.layers\\.\\d+\\.mlp\\.(gate_proj|up_proj|down_proj)\\.weight)$\"),\n",
        "    re.compile(r\"^(transformer\\.h\\.\\d+\\.attn\\.(q_proj|k_proj|v_proj|o_proj)\\.weight)$\"),\n",
        "    re.compile(r\"^(transformer\\.h\\.\\d+\\.mlp\\.(gate_proj|up_proj|down_proj)\\.weight)$\"),\n",
        "    # Sometimes names differ (wq/wk/wv/wo):\n",
        "    re.compile(r\"^(model\\.layers\\.\\d+\\.self_attn\\.(wq|wk|wv|wo)\\.weight)$\"),\n",
        "    re.compile(r\"^(transformer\\.h\\.\\d+\\.attn\\.(wq|wk|wv|wo)\\.weight)$\"),\n",
        "]\n",
        "\n",
        "# Things you typically DO NOT want in 4-bit float packing:\n",
        "# - norms, biases, rotary emb, small vectors, scalars, etc.\n",
        "QWEN_BLACKLIST_SUBSTR = [\n",
        "    \".bias\",\n",
        "    \".norm\", \"layernorm\", \"ln_\", \".ln\",\n",
        "    \"rotary\", \"rope\",\n",
        "    \"inv_freq\",\n",
        "    \"cos_cached\", \"sin_cached\",\n",
        "]\n",
        "\n",
        "# Embeddings/lm_head are big 2D matrices but often better left BF16/FP8 (your call).\n",
        "QWEN_BIG_MATS_SPECIAL = [\n",
        "    \"model.embed_tokens.weight\",\n",
        "    \"transformer.wte.weight\",\n",
        "    \"lm_head.weight\",\n",
        "    \"model.lm_head.weight\",\n",
        "]\n",
        "\n",
        "\n",
        "def _looks_like_qwen(keys: List[str]) -> bool:\n",
        "    # Heuristic: if it contains model.layers.N.self_attn.* or transformer.h.N.attn.*\n",
        "    for k in keys:\n",
        "        if \"model.layers.\" in k and \".self_attn.\" in k:\n",
        "            return True\n",
        "        if \"transformer.h.\" in k and \".attn.\" in k:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def _matches_any_allowlist(k: str, allowlist: List[re.Pattern]) -> bool:\n",
        "    return any(p.match(k) is not None for p in allowlist)\n",
        "\n",
        "\n",
        "def _is_blacklisted(k: str, blacklist_substr: List[str]) -> bool:\n",
        "    lk = k.lower()\n",
        "    return any(s in lk for s in blacklist_substr)\n",
        "\n",
        "\n",
        "def _is_2d_weight(k: str, t: torch.Tensor) -> bool:\n",
        "    return k.endswith(\".weight\") and t.ndim == 2 and t.dtype.is_floating_point\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Quant packing (practical, loader-friendly)\n",
        "# - NVFP4 (approx): signed int4 per-row + per-row scale, packed 2 nibbles per byte\n",
        "# - NVFP8/FP8 (approx): signed int8 per-row + per-row scale\n",
        "#\n",
        "# Notes:\n",
        "# - This produces compact safetensors + explicit metadata.\n",
        "# - If you already have a custom runtime that reads _quantization_metadata,\n",
        "#   you can map these qweight/scale blobs into your kernels.\n",
        "# ============================================================\n",
        "\n",
        "def _to_2s_comp_4bit(q: torch.Tensor) -> torch.Tensor:\n",
        "    # q int8 in [-8, 7] -> uint8 nibble two's complement\n",
        "    q = q.to(torch.int16)\n",
        "    q = torch.where(q < 0, q + 16, q)\n",
        "    return (q & 0xF).to(torch.uint8)\n",
        "\n",
        "def _pack_int4_nibbles(q_nibble_u8: torch.Tensor) -> torch.Tensor:\n",
        "    # q_nibble_u8 shape [R, C] each in [0..15] -> packed [R, ceil(C/2)]\n",
        "    R, C = q_nibble_u8.shape\n",
        "    C2 = (C + 1) // 2\n",
        "    out = torch.empty((R, C2), dtype=torch.uint8, device=q_nibble_u8.device)\n",
        "    lo = q_nibble_u8[:, 0::2]\n",
        "    hi = q_nibble_u8[:, 1::2]\n",
        "    out[:, :lo.shape[1]] = lo\n",
        "    out[:, :hi.shape[1]] |= (hi << 4)\n",
        "    # if odd C, the last high nibble stays 0\n",
        "    return out\n",
        "\n",
        "def quantize_nvfp4_per_row(w: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Approx NVFP4:\n",
        "      per-row scale = max(abs(w_row)) / 7\n",
        "      q = round(w/scale) clipped to [-8..7] (we reserve -8 too)\n",
        "      store q as packed int4 + scale\n",
        "    \"\"\"\n",
        "    if w.ndim != 2:\n",
        "        raise ValueError(\"nvfp4 expects 2D weight\")\n",
        "\n",
        "    w = w.detach()\n",
        "    # work in fp32 for stability, then store scale in fp16\n",
        "    wf = w.float()\n",
        "    max_abs = wf.abs().amax(dim=1).clamp_min(1e-12)  # [R]\n",
        "    scale = (max_abs / 7.0).to(torch.float16)        # [R]\n",
        "    q = torch.round(wf / scale.unsqueeze(1)).clamp(-8, 7).to(torch.int8)  # [R, C]\n",
        "\n",
        "    q_nib = _to_2s_comp_4bit(q)                       # [R, C] uint8 nibbles\n",
        "    q_packed = _pack_int4_nibbles(q_nib).contiguous() # [R, ceil(C/2)] uint8\n",
        "    return q_packed.cpu(), scale.cpu()\n",
        "\n",
        "def quantize_int8_per_row(w: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
        "    \"\"\"\n",
        "    Per-row int8 with scale:\n",
        "      scale = max(abs)/127\n",
        "      q = round(w/scale) clipped [-127..127]\n",
        "    \"\"\"\n",
        "    if w.ndim != 2:\n",
        "        raise ValueError(\"int8-per-row expects 2D weight\")\n",
        "    w = w.detach()\n",
        "    wf = w.float()\n",
        "    max_abs = wf.abs().amax(dim=1).clamp_min(1e-12)\n",
        "    scale = (max_abs / 127.0).to(torch.float16)\n",
        "    q = torch.round(wf / scale.unsqueeze(1)).clamp(-127, 127).to(torch.int8)\n",
        "    return q.contiguous().cpu(), scale.cpu()\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Conversion policy\n",
        "# ============================================================\n",
        "\n",
        "@dataclass\n",
        "class ConvertConfig:\n",
        "    model_type: str = \"auto\"  # auto | qwen | generic\n",
        "    # What to do when a key is blacklist-matched but still a 2D .weight matrix:\n",
        "    # - \"bf16\": keep as BF16\n",
        "    # - \"fp8\": store as fp8-like (int8+scale) with fp8_format metadata\n",
        "    blacklisted_2d_mode: str = \"fp8\"  # bf16 | fp8\n",
        "\n",
        "    # Fallback chain for *non-blacklisted* 2D weights:\n",
        "    # Attempt formats in this order until one succeeds:\n",
        "    # nvfp4 -> nvfp8 -> fp8 -> bf16 (your requested behavior)\n",
        "    fallback_chain: Tuple[str, ...] = (\"nvfp4\", \"nvfp8\", \"fp8\", \"bf16\")\n",
        "\n",
        "    # Choose ONE fp8 format label (no auto-fallback between them)\n",
        "    fallback_fp8_format: str = \"e4m3fn\"  # e4m3fn | e5m2\n",
        "\n",
        "    # If True, quantize any 2D .weight not allowlisted (unless blacklisted rules apply)\n",
        "    quantize_unmatched_2d: bool = True\n",
        "\n",
        "    # Allowlist only applies strongly to qwen mode; in generic mode we quantize all 2D weights unless blacklisted.\n",
        "    qwen_allowlist_only: bool = False  # if True, only allowlisted qwen blocks get nvfp4/nvfp8/fp8 attempts\n",
        "\n",
        "\n",
        "def _validate_config(cfg: ConvertConfig) -> None:\n",
        "    if cfg.blacklisted_2d_mode not in (\"bf16\", \"fp8\"):\n",
        "        raise ValueError(\"blacklisted_2d_mode must be bf16 or fp8\")\n",
        "    for f in cfg.fallback_chain:\n",
        "        if f not in (\"nvfp4\", \"nvfp8\", \"fp8\", \"bf16\"):\n",
        "            raise ValueError(\"fallback_chain entries must be nvfp4|nvfp8|fp8|bf16\")\n",
        "    if cfg.fallback_fp8_format not in (\"e4m3fn\", \"e5m2\"):\n",
        "        raise ValueError(\"fallback_fp8_format must be e4m3fn or e5m2\")\n",
        "\n",
        "\n",
        "def _attempt_quant(\n",
        "    fmt: str,\n",
        "    key: str,\n",
        "    w: torch.Tensor,\n",
        "    fp8_format: str,\n",
        ") -> Tuple[str, Dict[str, torch.Tensor], Dict]:\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      chosen_fmt,\n",
        "      out_tensors (new tensors to write),\n",
        "      layer_meta (json-serializable metadata for this layer)\n",
        "    \"\"\"\n",
        "    R, C = w.shape\n",
        "    if fmt == \"nvfp4\":\n",
        "        # pragmatic constraints you can tighten if your runtime requires block alignment\n",
        "        if C < 8:\n",
        "            raise RuntimeError(\"nvfp4: too few cols\")\n",
        "        qweight, scale = quantize_nvfp4_per_row(w)\n",
        "        return \"nvfp4\", {\n",
        "            f\"{key}.__qweight\": qweight,\n",
        "            f\"{key}.__scale\": scale,\n",
        "        }, {\n",
        "            \"format\": \"nvfp4\",\n",
        "            \"orig_shape\": [R, C],\n",
        "            \"qweight\": f\"{key}.__qweight\",\n",
        "            \"scale\": f\"{key}.__scale\",\n",
        "            \"pack\": \"int4_2scomp_per_row\",\n",
        "        }\n",
        "\n",
        "    if fmt == \"nvfp8\":\n",
        "        # treat nvfp8 as fp8-like int8+scale but label as nvfp8 for your pipeline\n",
        "        qweight, scale = quantize_int8_per_row(w)\n",
        "        return \"nvfp8\", {\n",
        "            f\"{key}.__qweight\": qweight,\n",
        "            f\"{key}.__scale\": scale,\n",
        "        }, {\n",
        "            \"format\": \"nvfp8\",\n",
        "            \"orig_shape\": [R, C],\n",
        "            \"qweight\": f\"{key}.__qweight\",\n",
        "            \"scale\": f\"{key}.__scale\",\n",
        "            \"pack\": \"int8_per_row\",\n",
        "            \"fp8_format\": fp8_format,\n",
        "        }\n",
        "\n",
        "    if fmt == \"fp8\":\n",
        "        qweight, scale = quantize_int8_per_row(w)\n",
        "        return \"fp8\", {\n",
        "            f\"{key}.__qweight\": qweight,\n",
        "            f\"{key}.__scale\": scale,\n",
        "        }, {\n",
        "            \"format\": \"fp8\",\n",
        "            \"orig_shape\": [R, C],\n",
        "            \"qweight\": f\"{key}.__qweight\",\n",
        "            \"scale\": f\"{key}.__scale\",\n",
        "            \"pack\": \"int8_per_row\",\n",
        "            \"fp8_format\": fp8_format,\n",
        "        }\n",
        "\n",
        "    if fmt == \"bf16\":\n",
        "        return \"bf16\", {key: w.to(torch.bfloat16).cpu().contiguous()}, {\"format\": \"bf16\", \"orig_shape\": [R, C]}\n",
        "\n",
        "    raise ValueError(f\"Unknown fmt: {fmt}\")\n",
        "\n",
        "\n",
        "def convert_safetensors(\n",
        "    in_path: str,\n",
        "    out_path: str,\n",
        "    cfg: ConvertConfig,\n",
        ") -> Dict:\n",
        "    _validate_config(cfg)\n",
        "\n",
        "    tensors = load_file(in_path)  # CPU tensors\n",
        "    keys = list(tensors.keys())\n",
        "\n",
        "    # Auto model detection\n",
        "    model_type = cfg.model_type\n",
        "    if model_type == \"auto\":\n",
        "        model_type = \"qwen\" if _looks_like_qwen(keys) else \"generic\"\n",
        "\n",
        "    out: Dict[str, torch.Tensor] = {}\n",
        "    layers_meta: Dict[str, Dict] = {}\n",
        "\n",
        "    # Preserve original top-level metadata if present (safetensors doesn't expose it via load_file),\n",
        "    # so we only write our own metadata.\n",
        "    stats = {\n",
        "        \"model_type\": model_type,\n",
        "        \"counts\": {\"nvfp4\": 0, \"nvfp8\": 0, \"fp8\": 0, \"bf16\": 0, \"kept\": 0, \"skipped\": 0},\n",
        "        \"total_keys_in\": len(keys),\n",
        "        \"total_keys_out\": 0,\n",
        "    }\n",
        "\n",
        "    for k, t in tensors.items():\n",
        "        # If it's not a floating tensor or not 2D weight, keep as-is (unless you want to cast)\n",
        "        if not (t.dtype.is_floating_point and t.ndim >= 1):\n",
        "            out[k] = t.contiguous()\n",
        "            stats[\"counts\"][\"kept\"] += 1\n",
        "            continue\n",
        "\n",
        "        # Handle 2D weight matrices\n",
        "        if _is_2d_weight(k, t):\n",
        "            is_black = _is_blacklisted(k, QWEN_BLACKLIST_SUBSTR if model_type == \"qwen\" else [])\n",
        "            is_special_big = (k in QWEN_BIG_MATS_SPECIAL) if model_type == \"qwen\" else False\n",
        "\n",
        "            # In qwen mode: allowlist detection for the \"layered model\" blocks\n",
        "            allow = True\n",
        "            if model_type == \"qwen\":\n",
        "                allow = _matches_any_allowlist(k, QWEN_ALLOWLIST_RE)\n",
        "                if cfg.qwen_allowlist_only and (not allow) and (not is_special_big):\n",
        "                    # keep unmatched 2D weights (unless you turned on quantize_unmatched_2d)\n",
        "                    if not cfg.quantize_unmatched_2d:\n",
        "                        out[k] = t.to(torch.bfloat16).contiguous()\n",
        "                        stats[\"counts\"][\"bf16\"] += 1\n",
        "                        continue\n",
        "\n",
        "            # Blacklisted 2D weights get separate handling\n",
        "            if is_black or is_special_big:\n",
        "                if cfg.blacklisted_2d_mode == \"bf16\":\n",
        "                    out[k] = t.to(torch.bfloat16).contiguous()\n",
        "                    layers_meta[k] = {\"format\": \"bf16\", \"reason\": \"blacklisted_or_special\"}\n",
        "                    stats[\"counts\"][\"bf16\"] += 1\n",
        "                    continue\n",
        "                else:\n",
        "                    # fp8 for blacklisted 2D weights\n",
        "                    chosen, new_tensors, meta = _attempt_quant(\"fp8\", k, t, cfg.fallback_fp8_format)\n",
        "                    out.update(new_tensors)\n",
        "                    layers_meta[k] = {**meta, \"reason\": \"blacklisted_or_special\"}\n",
        "                    stats[\"counts\"][chosen] += 1\n",
        "                    continue\n",
        "\n",
        "            # Non-blacklisted 2D: try fallback chain (nvfp4 -> nvfp8 -> fp8 -> bf16)\n",
        "            last_err: Optional[str] = None\n",
        "            done = False\n",
        "            for fmt in cfg.fallback_chain:\n",
        "                try:\n",
        "                    chosen, new_tensors, meta = _attempt_quant(fmt, k, t, cfg.fallback_fp8_format)\n",
        "                    out.update(new_tensors)\n",
        "                    layers_meta[k] = meta\n",
        "                    stats[\"counts\"][chosen] += 1\n",
        "                    done = True\n",
        "                    break\n",
        "                except Exception as e:\n",
        "                    last_err = f\"{type(e).__name__}: {e}\"\n",
        "                    continue\n",
        "\n",
        "            if not done:\n",
        "                # final safety: keep bf16\n",
        "                out[k] = t.to(torch.bfloat16).contiguous()\n",
        "                layers_meta[k] = {\"format\": \"bf16\", \"fallback_from_error\": last_err}\n",
        "                stats[\"counts\"][\"bf16\"] += 1\n",
        "\n",
        "            continue\n",
        "\n",
        "        # Not a 2D weight: keep, but avoid fp32 bloat\n",
        "        # (you can change this to keep original dtype if you prefer)\n",
        "        if t.dtype == torch.float32:\n",
        "            out[k] = t.to(torch.bfloat16).contiguous()\n",
        "            stats[\"counts\"][\"bf16\"] += 1\n",
        "        else:\n",
        "            out[k] = t.contiguous()\n",
        "            stats[\"counts\"][\"kept\"] += 1\n",
        "\n",
        "    quant_meta = {\n",
        "        \"format_version\": \"1.1\",\n",
        "        \"model_type\": model_type,\n",
        "        \"fallback_chain\": list(cfg.fallback_chain),\n",
        "        \"blacklisted_2d_mode\": cfg.blacklisted_2d_mode,\n",
        "        \"fp8_format\": cfg.fallback_fp8_format,\n",
        "        \"layers\": layers_meta,\n",
        "    }\n",
        "\n",
        "    metadata = {\n",
        "        \"_quantization_metadata\": json.dumps(quant_meta, ensure_ascii=False),\n",
        "    }\n",
        "\n",
        "    os.makedirs(os.path.dirname(out_path) or \".\", exist_ok=True)\n",
        "    save_file(out, out_path, metadata=metadata)\n",
        "\n",
        "    stats[\"total_keys_out\"] = len(out.keys())\n",
        "    return stats\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# CLI\n",
        "# ============================================================\n",
        "\n",
        "def _parse_args() -> argparse.Namespace:\n",
        "    p = argparse.ArgumentParser(description=\"Convert safetensors weights to nvfp4/nvfp8/fp8/bf16 with metadata.\")\n",
        "    p.add_argument(\"--in\", dest=\"in_path\", required=True, help=\"Input .safetensors\")\n",
        "    p.add_argument(\"--out\", dest=\"out_path\", required=True, help=\"Output .safetensors\")\n",
        "\n",
        "    p.add_argument(\"--model\", dest=\"model_type\", default=\"auto\", choices=[\"auto\", \"qwen\", \"generic\"])\n",
        "    p.add_argument(\"--blacklisted-2d-mode\", default=\"fp8\", choices=[\"bf16\", \"fp8\"])\n",
        "    p.add_argument(\"--fallback-chain\", default=\"nvfp4,nvfp8,fp8,bf16\", help=\"Comma list: nvfp4,nvfp8,fp8,bf16\")\n",
        "    p.add_argument(\"--fp8-format\", default=\"e4m3fn\", choices=[\"e4m3fn\", \"e5m2\"])\n",
        "    p.add_argument(\"--quantize-unmatched-2d\", action=\"store_true\", help=\"Quantize unmatched 2D weights too (default True in code)\")\n",
        "    p.add_argument(\"--no-quantize-unmatched-2d\", action=\"store_true\", help=\"Do not quantize unmatched 2D weights\")\n",
        "    p.add_argument(\"--qwen-allowlist-only\", action=\"store_true\", help=\"In qwen mode, only quantize allowlisted blocks (+ specials handled separately)\")\n",
        "    return p.parse_args()\n",
        "\n",
        "\n",
        "def main() -> None:\n",
        "    args = _parse_args()\n",
        "    chain = tuple([s.strip() for s in args.fallback_chain.split(\",\") if s.strip()])\n",
        "    cfg = ConvertConfig(\n",
        "        model_type=args.model_type,\n",
        "        blacklisted_2d_mode=args.blacklisted_2d_mode,\n",
        "        fallback_chain=chain,\n",
        "        fallback_fp8_format=args.fp8_format,\n",
        "        quantize_unmatched_2d=(False if args.no_quantize_unmatched_2d else True),\n",
        "        qwen_allowlist_only=args.qwen_allowlist_only,\n",
        "    )\n",
        "    stats = convert_safetensors(args.in_path, args.out_path, cfg)\n",
        "    print(json.dumps(stats, indent=2))\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "id": "N_n4SbiQiQyi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kopQcgwViQyi"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title Convert settings\n",
        "# =========================\n",
        "# Cell: Convert (custom output filename)\n",
        "# =========================\n",
        "import os\n",
        "from convert_nvfp4 import convert_safetensors, ConvertConfig\n",
        "\n",
        "# ---- set your input path ----\n",
        "IN_PATH = downloaded_path\n",
        "\n",
        "# ---- custom output naming ----\n",
        "OUT_DIR = r\"/content/converted\"\n",
        "CUSTOM_BASENAME = \"qwen_layered_nvfp4_2dBF16\"  # <-- change\n",
        "OUT_PATH = os.path.join(OUT_DIR, f\"{CUSTOM_BASENAME}.safetensors\")\n",
        "\n",
        "cfg = ConvertConfig(\n",
        "    model_type=\"qwen\",                       # auto | qwen | generic\n",
        "    blacklisted_2d_mode=\"bf16\",               # bf16 | fp8\n",
        "    fallback_chain=(\"nvfp4\", \"nvfp8\", \"fp8\", \"bf16\"),  # nvfp4 -> nvfp8 -> fp8 -> bf16\n",
        "    fallback_fp8_format=\"e4m3fn\",            # e4m3fn | e5m2 (NO auto fallback between them)\n",
        "    quantize_unmatched_2d=True,              # tries the chain for leftover 2D weights too\n",
        "    qwen_allowlist_only=False,               # set True if you ONLY want the allowlisted Qwen layer blocks\n",
        ")\n",
        "\n",
        "stats = convert_safetensors(IN_PATH, OUT_PATH, cfg)\n",
        "print(\"Wrote:\", OUT_PATH)\n",
        "print(stats)"
      ],
      "id": "kopQcgwViQyi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbA917ZeiQyj"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title üîê Hugging Face login (token)\n",
        "from getpass import getpass\n",
        "HF_TOKEN = getpass(\"Paste your Hugging Face token (write access):\").strip()\n",
        "assert HF_TOKEN, \"Token is required\"\n",
        "print(\"Token received (not printing it).\")\n"
      ],
      "id": "YbA917ZeiQyj"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üèóÔ∏è Create repo 'quanttesting' and upload NVFP4 model\n",
        "import os\n",
        "from huggingface_hub import HfApi\n",
        "output_path = OUT_PATH\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "who = api.whoami()\n",
        "username = who[\"name\"]\n",
        "\n",
        "repo_name = \"YOUR repo HERE\" # change\n",
        "repo_id = f\"{username}/{repo_name}\"\n",
        "\n",
        "api.create_repo(repo_id=repo_id, repo_type=\"model\", exist_ok=True)\n",
        "print(\"Repo:\", repo_id)\n",
        "\n",
        "# Upload the NVFP4 safetensors (uses LFS automatically for large files)\n",
        "api.upload_file(\n",
        "    path_or_fileobj=output_path,\n",
        "    path_in_repo=os.path.basename(output_path),\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        "    commit_message=\"Upload NVFP4-converted\",\n",
        ")\n",
        "print(\"‚úÖ Upload complete.\")\n"
      ],
      "metadata": {
        "id": "atGHZ5IF3A88"
      },
      "id": "atGHZ5IF3A88",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "G4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
