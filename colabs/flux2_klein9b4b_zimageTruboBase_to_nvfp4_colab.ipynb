{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#@title üß∞ Pre-convert setup (installs + folders)\n",
        "import os, sys, platform, torch\n",
        "\n",
        "print(\"Python:\", sys.version)\n",
        "print(\"Platform:\", platform.platform())\n",
        "print(\"Torch:\", torch.__version__)\n",
        "print(\"CUDA available:\", torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    print(\"GPU:\", torch.cuda.get_device_name(0))\n",
        "    print(\"Compute capability:\", torch.cuda.get_device_capability(0))\n",
        "    print(\"CUDA runtime:\", torch.version.cuda)\n",
        "\n",
        "!pip -q install -U safetensors tqdm requests==2.32.4 huggingface_hub hf_transfer\n",
        "!pip -q install -U comfy-kitchen\n",
        "\n",
        "# optional: faster Hugging Face transfers\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
        "print(\"HF_HUB_ENABLE_HF_TRANSFER=1\")\n",
        "\n",
        "MODEL_DIR = \"/content/models\"\n",
        "os.makedirs(MODEL_DIR, exist_ok=True)\n",
        "print(\"MODEL_DIR:\", MODEL_DIR)"
      ],
      "metadata": {
        "id": "HgOrhN4uVrYk"
      },
      "id": "HgOrhN4uVrYk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o11YactYiQyi",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title üì• Download BF16 model from Hugging Face\n",
        "from huggingface_hub import hf_hub_download\n",
        "\n",
        "REPO_ID = \"black-forest-labs/FLUX.2-klein-4B\" # or black-forest-labs/FLUX.2-klein-4B | artokun/blzib\n",
        "FILENAME = \"flux-2-klein-4b.safetensors\" # or flux-2-klein-4b.safetensors | blzib.safetensors\n",
        "\n",
        "input_path = hf_hub_download(repo_id=REPO_ID, filename=FILENAME)\n",
        "print(\"Downloaded to:\", input_path)\n",
        "# After hf_hub_download(...)\n",
        "downloaded_path = input_path  # so key-check + convert cells that expect downloaded_path still work\n",
        "print(\"downloaded_path set to:\", downloaded_path)\n"
      ],
      "id": "o11YactYiQyi"
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üì• Download from Civitai (modelId + modelVersionId) slower\n",
        "import os, re, requests\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# ===== YOU SET THESE =====\n",
        "CIVITAI_MODEL_ID = \"\"          # optional (info lookup only)\n",
        "CIVITAI_VERSION_ID = \"\"        # REQUIRED\n",
        "CIVITAI_API_TOKEN = \"\"         # optional; needed for models requiring login\n",
        "FILENAME_OVERRIDE = \"\"         # optional\n",
        "# =========================\n",
        "\n",
        "def _filename_from_cd(cd: str):\n",
        "    if not cd:\n",
        "        return None\n",
        "    m = re.search(r'filename\\*?=(?:UTF-8\\'\\')?\"?([^\\\";]+)\"?', cd, flags=re.IGNORECASE)\n",
        "    return m.group(1) if m else None\n",
        "\n",
        "def civitai_download(model_version_id: str, out_dir: str, token: str = \"\", filename_override: str = \"\") -> str:\n",
        "    url = f\"https://civitai.com/api/download/models/{model_version_id}\"\n",
        "    # token via query string is explicitly supported\n",
        "    if token:\n",
        "        url += f\"?token={token}\"\n",
        "\n",
        "    with requests.get(url, stream=True, allow_redirects=True, timeout=(10, 120)) as r:\n",
        "        r.raise_for_status()\n",
        "        total = int(r.headers.get(\"Content-Length\", \"0\") or \"0\")\n",
        "        cd = r.headers.get(\"Content-Disposition\", \"\")\n",
        "\n",
        "        fname = filename_override.strip() or _filename_from_cd(cd) or f\"civitai_{model_version_id}.safetensors\"\n",
        "        out_path = os.path.join(out_dir, fname)\n",
        "\n",
        "        pbar = tqdm(total=total if total > 0 else None, unit=\"B\", unit_scale=True, desc=f\"Downloading {fname}\")\n",
        "        with open(out_path, \"wb\") as f:\n",
        "            for chunk in r.iter_content(chunk_size=1024 * 1024):\n",
        "                if chunk:\n",
        "                    f.write(chunk)\n",
        "                    pbar.update(len(chunk))\n",
        "        pbar.close()\n",
        "\n",
        "    return out_path\n",
        "\n",
        "assert CIVITAI_VERSION_ID.strip(), \"Set CIVITAI_VERSION_ID (modelVersionId).\"\n",
        "downloaded_path = civitai_download(CIVITAI_VERSION_ID.strip(), MODEL_DIR, CIVITAI_API_TOKEN.strip(), FILENAME_OVERRIDE.strip())\n",
        "print(\"‚úÖ Downloaded:\", downloaded_path)"
      ],
      "metadata": {
        "id": "nUZ3F6bZV9SB",
        "cellView": "form"
      },
      "id": "nUZ3F6bZV9SB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title üßæ Key check: list safetensors keys (prints + saves full list) For adding more support\n",
        "import os, safetensors\n",
        "\n",
        "assert downloaded_path.endswith(\".safetensors\"), f\"Not a .safetensors file: {downloaded_path}\"\n",
        "\n",
        "with safetensors.safe_open(downloaded_path, framework=\"pt\") as f:\n",
        "    keys = list(f.keys())\n",
        "    meta = f.metadata() or {}\n",
        "\n",
        "print(\"‚úÖ Key count:\", len(keys))\n",
        "print(\"‚úÖ First 120 keys:\")\n",
        "for k in keys[:120]:\n",
        "    print(k)\n",
        "\n",
        "keys_txt = \"/content/model_keys.txt\"\n",
        "with open(keys_txt, \"w\", encoding=\"utf-8\") as w:\n",
        "    for k in keys:\n",
        "        w.write(k + \"\\n\")\n",
        "\n",
        "print(\"Saved full key list to:\", keys_txt)\n",
        "print(\"Metadata keys:\", list(meta.keys())[:40])\n",
        "print(\"Has 'model.diffusion_model.' prefix?\", any(\"model.diffusion_model.\" in k for k in keys))"
      ],
      "metadata": {
        "id": "QHpHDLi7WEZE",
        "cellView": "form"
      },
      "id": "QHpHDLi7WEZE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N_n4SbiQiQyi"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title üß± Write converter module to disk (convert_nvfp4.py)\n",
        "%%writefile convert_nvfp4.py\n",
        "\"\"\"\n",
        "convert_nvfp4.py\n",
        "\n",
        "Supports:\n",
        "- Flux.2-Klein-9b / Flux.2-Klein-4b  (txt_attn_mode switching)\n",
        "- Z-Image-Turbo / Z-Image-Base\n",
        "\n",
        "FP8 formats:\n",
        "- \"e4m3fn\" -> float8_e4m3fn\n",
        "- \"e5m2\"   -> float8_e5m2\n",
        "\n",
        "New behaviors (apply to all model types):\n",
        "- blacklisted_2d_mode:\n",
        "    * \"bf16\": keep blacklist-matched tensors BF16\n",
        "    * \"fp8\" : if blacklist-matched tensor is a 2D \".weight\", store it as FP8 (chosen fp8_format)\n",
        "- nvfp4_fail_fallback:\n",
        "    * \"bf16\": NVFP4 failure -> BF16\n",
        "    * \"fp8\" : NVFP4 failure -> FP8 (chosen fp8_format), else BF16 if FP8 also fails\n",
        "\n",
        "Flux Klein txt_attn_mode:\n",
        "- \"nvfp4\" : try NVFP4 then fallback via nvfp4_fail_fallback\n",
        "- \"bf16\"  : keep BF16\n",
        "- \"fp8\"   : store txt_attn 2D weights as FP8 (chosen fp8_format)\n",
        "\n",
        "FP8 checkpoint fields:\n",
        "- store {base}.weight_scale and {base}.input_scale as float32 scalars.\n",
        "\"\"\"\n",
        "\n",
        "from __future__ import annotations\n",
        "\n",
        "import os\n",
        "import json\n",
        "from collections import OrderedDict\n",
        "from typing import Dict, Any, Tuple, List\n",
        "\n",
        "import torch\n",
        "import safetensors\n",
        "import safetensors.torch\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "try:\n",
        "    import comfy_kitchen as ck\n",
        "    from comfy_kitchen.tensor import TensorCoreNVFP4Layout\n",
        "except Exception as e:\n",
        "    ck = None\n",
        "    TensorCoreNVFP4Layout = None\n",
        "    _IMPORT_ERR = e\n",
        "\n",
        "SUPPORTED_TYPES = (\"Flux.2-Klein-9b\", \"Flux.2-Klein-4b\", \"Z-Image-Turbo\", \"Z-Image-Base\")\n",
        "\n",
        "_FP8_DTYPE = {\n",
        "    \"e4m3fn\": torch.float8_e4m3fn,\n",
        "    \"e5m2\": torch.float8_e5m2,\n",
        "}\n",
        "_FP8_META_FORMAT = {\n",
        "    \"e4m3fn\": \"float8_e4m3fn\",\n",
        "    \"e5m2\": \"float8_e5m2\",\n",
        "}\n",
        "\n",
        "def _sm_str() -> str:\n",
        "    if not torch.cuda.is_available():\n",
        "        return \"CPU\"\n",
        "    major, minor = torch.cuda.get_device_capability()\n",
        "    return f\"SM{major}{minor}\"\n",
        "\n",
        "def _profile(model_type: str, match_official_flux9b: bool = True) -> Tuple[List[str], List[str]]:\n",
        "    \"\"\"\n",
        "    Returns (BLACKLIST, EXTRA_BF16)\n",
        "    EXTRA_BF16 always stays BF16.\n",
        "    \"\"\"\n",
        "    if model_type == \"Z-Image-Base\":\n",
        "        blacklist = [\n",
        "            \"attention.out\", \"layers.0.\", \"layers.29.\", \"adaLN_modulation\", \"norm\",\n",
        "            \"final_layer\", \"cap_embedder\", \"x_embedder\", \"noise_refiner\", \"context_refiner\", \"t_embedder\"\n",
        "        ]\n",
        "        return blacklist, []\n",
        "\n",
        "    if model_type == \"Z-Image-Turbo\":\n",
        "        blacklist = [\"cap_embedder\", \"x_embedder\", \"noise_refiner\", \"context_refiner\", \"t_embedder\", \"final_layer\"]\n",
        "        return blacklist, []\n",
        "\n",
        "    if model_type in (\"Flux.2-Klein-9b\", \"Flux.2-Klein-4b\"):\n",
        "        blacklist = [\n",
        "            \"bias\",\n",
        "            \"img_in\", \"txt_in\", \"time_in\", \"vector_in\", \"guidance_in\",\n",
        "            \"final_layer\", \"class_embedding\",\n",
        "            \"single_stream_modulation\", \"double_stream_modulation_img\", \"double_stream_modulation_txt\",\n",
        "        ]\n",
        "        extra_bf16 = []\n",
        "        if match_official_flux9b and model_type == \"Flux.2-Klein-9b\":\n",
        "            extra_bf16 = [\"double_blocks.0.img_attn.qkv\", \"double_blocks.0.txt_attn.qkv\"]\n",
        "        return blacklist, extra_bf16\n",
        "\n",
        "    raise ValueError(f\"Unsupported model_type: {model_type}\")\n",
        "\n",
        "def _is_2d_weight(k: str, v: torch.Tensor) -> bool:\n",
        "    return (v.ndim == 2) and k.endswith(\".weight\")\n",
        "\n",
        "def _is_txt_attn_weight(k: str, v: torch.Tensor) -> bool:\n",
        "    return (\"txt_attn\" in k) and _is_2d_weight(k, v)\n",
        "\n",
        "def _base_keys(k: str) -> Tuple[str, str]:\n",
        "    base_k_file = k[:-len(\".weight\")]\n",
        "    if \"model.diffusion_model.\" in base_k_file:\n",
        "        base_k_meta = base_k_file.split(\"model.diffusion_model.\")[-1]\n",
        "    else:\n",
        "        base_k_meta = base_k_file\n",
        "    return base_k_file, base_k_meta\n",
        "\n",
        "def _fp8_scale(t: torch.Tensor, fp8_dtype: torch.dtype,\n",
        "              strategy: str = \"absmax\",\n",
        "              percentile: float = 99.9,\n",
        "              slack: float = 1.05) -> torch.Tensor:\n",
        "    max_fp8 = torch.finfo(fp8_dtype).max\n",
        "    if strategy == \"absmax\":\n",
        "        a = torch.amax(t.abs())\n",
        "    elif strategy == \"absmax_slack\":\n",
        "        a = torch.amax(t.abs()) * float(slack)\n",
        "    elif strategy == \"percentile\":\n",
        "        a = torch.quantile(t.abs().flatten().float(), float(percentile) / 100.0)\n",
        "    else:\n",
        "        raise ValueError(\"fp8_scale_strategy must be absmax | absmax_slack | percentile\")\n",
        "    return (a / max_fp8).clamp(min=1e-12).float()\n",
        "\n",
        "def _fp8_pack(new_sd: Dict[str, torch.Tensor], quant_map: Dict[str, Any],\n",
        "              k: str, base_k_file: str, base_k_meta: str,\n",
        "              v_tensor: torch.Tensor,\n",
        "              fp8_dtype: torch.dtype, fp8_meta_fmt: str,\n",
        "              fp8_scale_strategy: str, fp8_percentile: float, fp8_slack: float) -> bool:\n",
        "    \"\"\"\n",
        "    Returns True if FP8 write succeeded.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        scale = _fp8_scale(v_tensor, fp8_dtype, strategy=fp8_scale_strategy, percentile=fp8_percentile, slack=fp8_slack)\n",
        "        q = ck.quantize_per_tensor_fp8(v_tensor, scale, fp8_dtype)\n",
        "        new_sd[k] = q.cpu()\n",
        "        new_sd[f\"{base_k_file}.weight_scale\"] = scale.float().cpu()                     # float32 scalar\n",
        "        new_sd[f\"{base_k_file}.input_scale\"]  = torch.ones((), dtype=torch.float32).cpu()  # float32 scalar default\n",
        "        quant_map[\"layers\"][base_k_meta] = {\"format\": fp8_meta_fmt}\n",
        "        return True\n",
        "    except Exception:\n",
        "        return False\n",
        "\n",
        "def _nvfp4_pack(new_sd: Dict[str, torch.Tensor], quant_map: Dict[str, Any],\n",
        "                base_k_file: str, base_k_meta: str,\n",
        "                v_tensor: torch.Tensor) -> None:\n",
        "    qdata, params = TensorCoreNVFP4Layout.quantize(v_tensor)\n",
        "    tensors = TensorCoreNVFP4Layout.state_dict_tensors(qdata, params)\n",
        "    for suffix, t in tensors.items():\n",
        "        new_sd[f\"{base_k_file}.weight{suffix}\"] = t.cpu()\n",
        "    quant_map[\"layers\"][base_k_meta] = {\"format\": \"nvfp4\"}\n",
        "\n",
        "def convert_to_nvfp4(\n",
        "    input_path: str,\n",
        "    output_path: str,\n",
        "    model_type: str,\n",
        "    device: str = \"cuda\",\n",
        "    match_official: bool = True,\n",
        "\n",
        "    # Flux Klein only:\n",
        "    txt_attn_mode: str = \"nvfp4\",            # \"nvfp4\" | \"bf16\" | \"fp8\"\n",
        "\n",
        "    # FP8 (used for txt_attn fp8, blacklisted fp8, and nvfp4 fallback fp8):\n",
        "    fp8_format: str = \"e4m3fn\",              # \"e4m3fn\" | \"e5m2\"\n",
        "    fp8_scale_strategy: str = \"absmax\",      # absmax | absmax_slack | percentile\n",
        "    fp8_slack: float = 1.05,\n",
        "    fp8_percentile: float = 99.9,\n",
        "\n",
        "    # New:\n",
        "    blacklisted_2d_mode: str = \"bf16\",       # \"bf16\" | \"fp8\"\n",
        "    nvfp4_fail_fallback: str = \"bf16\",       # \"bf16\" | \"fp8\"\n",
        "\n",
        "    verbose: bool = True,\n",
        ") -> Dict[str, Any]:\n",
        "    if ck is None or TensorCoreNVFP4Layout is None:\n",
        "        raise RuntimeError(f\"comfy-kitchen import failed: {_IMPORT_ERR}\\nInstall: pip install comfy-kitchen\")\n",
        "\n",
        "    if model_type not in SUPPORTED_TYPES:\n",
        "        raise ValueError(f\"model_type must be one of: {SUPPORTED_TYPES}\")\n",
        "\n",
        "    if device not in (\"cuda\", \"cpu\"):\n",
        "        raise ValueError(\"device must be 'cuda' or 'cpu'\")\n",
        "    if device == \"cuda\" and not torch.cuda.is_available():\n",
        "        raise RuntimeError(\"CUDA requested but torch.cuda.is_available() is False\")\n",
        "\n",
        "    if txt_attn_mode not in (\"nvfp4\", \"bf16\", \"fp8\"):\n",
        "        raise ValueError(\"txt_attn_mode must be nvfp4 | bf16 | fp8\")\n",
        "    if fp8_format not in _FP8_DTYPE:\n",
        "        raise ValueError(\"fp8_format must be e4m3fn | e5m2\")\n",
        "\n",
        "    if blacklisted_2d_mode not in (\"bf16\", \"fp8\"):\n",
        "        raise ValueError(\"blacklisted_2d_mode must be bf16 | fp8\")\n",
        "    if nvfp4_fail_fallback not in (\"bf16\", \"fp8\"):\n",
        "        raise ValueError(\"nvfp4_fail_fallback must be bf16 | fp8\")\n",
        "\n",
        "    fp8_dtype = _FP8_DTYPE[fp8_format]\n",
        "    fp8_meta  = _FP8_META_FORMAT[fp8_format]\n",
        "\n",
        "    blacklist, extra_bf16 = _profile(model_type, match_official_flux9b=match_official)\n",
        "\n",
        "    # stream read keys + metadata\n",
        "    with safetensors.safe_open(input_path, framework=\"pt\") as f:\n",
        "        keys = list(f.keys())\n",
        "        orig_meta = f.metadata() or {}\n",
        "\n",
        "    quant_map = {\"format_version\": \"1.0\", \"layers\": {}}\n",
        "    new_sd: Dict[str, torch.Tensor] = {}\n",
        "\n",
        "    if verbose:\n",
        "        print(f\"üîß Input:  {input_path}\")\n",
        "        print(f\"üíæ Output: {output_path}\")\n",
        "        print(f\"üß† Model:  {model_type}\")\n",
        "        print(f\"üß† Device: {device} ({_sm_str()})\")\n",
        "        print(f\"üéØ match_official={match_official} | extra_bf16={extra_bf16}\")\n",
        "        print(f\"üßä fp8_format={fp8_format} | blacklisted_2d_mode={blacklisted_2d_mode} | nvfp4_fail_fallback={nvfp4_fail_fallback}\")\n",
        "        if model_type.startswith(\"Flux.2-Klein\"):\n",
        "            print(f\"üéõÔ∏è txt_attn_mode={txt_attn_mode}\")\n",
        "        print(f\"üì¶ Keys: {len(keys)}\")\n",
        "\n",
        "    nvfp4_ok = nvfp4_fail = 0\n",
        "    fp8_ok = fp8_fail = 0\n",
        "    bf16_keep = 0\n",
        "\n",
        "    with safetensors.safe_open(input_path, framework=\"pt\") as f:\n",
        "        for k in tqdm(keys, desc=\"Converting\", unit=\"tensor\"):\n",
        "            v = f.get_tensor(k)\n",
        "\n",
        "            in_extra = any(s in k for s in extra_bf16)\n",
        "            in_blacklist = any(s in k for s in blacklist)\n",
        "\n",
        "            # EXTRA_BF16 always BF16\n",
        "            if in_extra:\n",
        "                new_sd[k] = v.to(dtype=torch.bfloat16)\n",
        "                bf16_keep += 1\n",
        "                continue\n",
        "\n",
        "            # Blacklist: optionally FP8 for 2D weights\n",
        "            if in_blacklist:\n",
        "                if blacklisted_2d_mode == \"fp8\" and _is_2d_weight(k, v):\n",
        "                    base_k_file, base_k_meta = _base_keys(k)\n",
        "                    v_tensor = v.to(device=device, dtype=torch.bfloat16)\n",
        "                    ok = _fp8_pack(new_sd, quant_map, k, base_k_file, base_k_meta, v_tensor,\n",
        "                                   fp8_dtype, fp8_meta, fp8_scale_strategy, fp8_percentile, fp8_slack)\n",
        "                    if ok:\n",
        "                        fp8_ok += 1\n",
        "                    else:\n",
        "                        fp8_fail += 1\n",
        "                        new_sd[k] = v.to(dtype=torch.bfloat16)\n",
        "                        bf16_keep += 1\n",
        "                    if device == \"cuda\":\n",
        "                        del v_tensor\n",
        "                    continue\n",
        "\n",
        "                new_sd[k] = v.to(dtype=torch.bfloat16)\n",
        "                bf16_keep += 1\n",
        "                continue\n",
        "\n",
        "            # Flux Klein: txt_attn special handling\n",
        "            if model_type.startswith(\"Flux.2-Klein\") and _is_txt_attn_weight(k, v):\n",
        "                if txt_attn_mode == \"bf16\":\n",
        "                    new_sd[k] = v.to(dtype=torch.bfloat16)\n",
        "                    bf16_keep += 1\n",
        "                    continue\n",
        "\n",
        "                base_k_file, base_k_meta = _base_keys(k)\n",
        "                v_tensor = v.to(device=device, dtype=torch.bfloat16)\n",
        "\n",
        "                if txt_attn_mode == \"fp8\":\n",
        "                    ok = _fp8_pack(new_sd, quant_map, k, base_k_file, base_k_meta, v_tensor,\n",
        "                                   fp8_dtype, fp8_meta, fp8_scale_strategy, fp8_percentile, fp8_slack)\n",
        "                    if ok:\n",
        "                        fp8_ok += 1\n",
        "                    else:\n",
        "                        fp8_fail += 1\n",
        "                        new_sd[k] = v.to(dtype=torch.bfloat16)\n",
        "                        bf16_keep += 1\n",
        "                    if device == \"cuda\":\n",
        "                        del v_tensor\n",
        "                    continue\n",
        "\n",
        "                # txt_attn_mode == nvfp4\n",
        "                try:\n",
        "                    _nvfp4_pack(new_sd, quant_map, base_k_file, base_k_meta, v_tensor)\n",
        "                    nvfp4_ok += 1\n",
        "                except Exception:\n",
        "                    nvfp4_fail += 1\n",
        "                    if nvfp4_fail_fallback == \"fp8\":\n",
        "                        ok = _fp8_pack(new_sd, quant_map, k, base_k_file, base_k_meta, v_tensor,\n",
        "                                       fp8_dtype, fp8_meta, fp8_scale_strategy, fp8_percentile, fp8_slack)\n",
        "                        if ok:\n",
        "                            fp8_ok += 1\n",
        "                        else:\n",
        "                            fp8_fail += 1\n",
        "                            new_sd[k] = v.to(dtype=torch.bfloat16)\n",
        "                            bf16_keep += 1\n",
        "                    else:\n",
        "                        new_sd[k] = v.to(dtype=torch.bfloat16)\n",
        "                        bf16_keep += 1\n",
        "                finally:\n",
        "                    if device == \"cuda\":\n",
        "                        del v_tensor\n",
        "                continue\n",
        "\n",
        "            # Default: NVFP4 for 2D weights; fallback if requested\n",
        "            if _is_2d_weight(k, v):\n",
        "                base_k_file, base_k_meta = _base_keys(k)\n",
        "                v_tensor = v.to(device=device, dtype=torch.bfloat16)\n",
        "                try:\n",
        "                    _nvfp4_pack(new_sd, quant_map, base_k_file, base_k_meta, v_tensor)\n",
        "                    nvfp4_ok += 1\n",
        "                except Exception:\n",
        "                    nvfp4_fail += 1\n",
        "                    if nvfp4_fail_fallback == \"fp8\":\n",
        "                        ok = _fp8_pack(new_sd, quant_map, k, base_k_file, base_k_meta, v_tensor,\n",
        "                                       fp8_dtype, fp8_meta, fp8_scale_strategy, fp8_percentile, fp8_slack)\n",
        "                        if ok:\n",
        "                            fp8_ok += 1\n",
        "                        else:\n",
        "                            fp8_fail += 1\n",
        "                            new_sd[k] = v.to(dtype=torch.bfloat16)\n",
        "                            bf16_keep += 1\n",
        "                    else:\n",
        "                        new_sd[k] = v.to(dtype=torch.bfloat16)\n",
        "                        bf16_keep += 1\n",
        "                finally:\n",
        "                    if device == \"cuda\":\n",
        "                        del v_tensor\n",
        "            else:\n",
        "                new_sd[k] = v.to(dtype=torch.bfloat16)\n",
        "                bf16_keep += 1\n",
        "\n",
        "    final_metadata = OrderedDict()\n",
        "    final_metadata[\"_quantization_metadata\"] = json.dumps(quant_map)\n",
        "    for mk, mv in orig_meta.items():\n",
        "        if mk not in final_metadata:\n",
        "            final_metadata[mk] = mv\n",
        "\n",
        "    safetensors.torch.save_file(new_sd, output_path, metadata=final_metadata)\n",
        "\n",
        "    total_bytes = os.path.getsize(output_path)\n",
        "    summary = {\n",
        "        \"model_type\": model_type,\n",
        "        \"input_path\": input_path,\n",
        "        \"output_path\": output_path,\n",
        "        \"output_gb\": round(total_bytes / (1024**3), 3),\n",
        "        \"nvfp4_ok\": nvfp4_ok,\n",
        "        \"nvfp4_fail\": nvfp4_fail,\n",
        "        \"fp8_ok\": fp8_ok,\n",
        "        \"fp8_fail\": fp8_fail,\n",
        "        \"bf16_kept\": bf16_keep,\n",
        "        \"sm\": _sm_str(),\n",
        "        \"fp8_format\": fp8_format,\n",
        "        \"blacklisted_2d_mode\": blacklisted_2d_mode,\n",
        "        \"nvfp4_fail_fallback\": nvfp4_fail_fallback,\n",
        "    }\n",
        "    print(\"‚úÖ Done:\", summary)\n",
        "    return summary"
      ],
      "id": "N_n4SbiQiQyi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kopQcgwViQyi"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title üç≥ Convert (custom output filename)\n",
        "import os\n",
        "from convert_nvfp4 import convert_to_nvfp4\n",
        "\n",
        "input_path = downloaded_path  # HF or Civitai download\n",
        "OUT_DIR = \"/content/models\"\n",
        "os.makedirs(OUT_DIR, exist_ok=True)\n",
        "\n",
        "MODEL_TYPE = \"Flux.2-Klein-4b\"  # \"Z-Image-Turbo\" | \"Z-Image-Base\" | \"Flux.2-Klein-9b\" | \"Flux.2-Klein-4b\"\n",
        "OUT_NAME  = \"FluxKlein4b_nvfp4\"  # change to whatever\n",
        "output_path = os.path.join(OUT_DIR, f\"{OUT_NAME}.safetensors\")\n",
        "\n",
        "# =========================\n",
        "# COMMON (applies to all)\n",
        "# =========================\n",
        "DEVICE = \"cuda\"\n",
        "MATCH_OFFICIAL = True\n",
        "\n",
        "# Pick ONE FP8 format (no chain):\n",
        "FP8_FORMAT = \"e4m3fn\"     # \"e4m3fn\" | \"e5m2\"\n",
        "\n",
        "# FP8 scaling knobs:\n",
        "FP8_SCALE_STRATEGY = \"absmax\"   # \"absmax\" | \"absmax_slack\" | \"percentile\"\n",
        "FP8_SLACK = 1.05\n",
        "FP8_PERCENTILE = 99.9\n",
        "\n",
        "# NVFP4 failure fallback:\n",
        "NVFP4_FAIL_FALLBACK = \"bf16\"    # \"bf16\" | \"fp8\"\n",
        "\n",
        "# =========================\n",
        "# Z-IMAGE options\n",
        "# =========================\n",
        "# What to do with blacklist-matched 2D \".weight\" tensors:\n",
        "ZIMAGE_BLACKLISTED_2D_MODE = \"bf16\"  # \"bf16\" | \"fp8\"\n",
        "\n",
        "# =========================\n",
        "# FLUX KLEIN options\n",
        "# =========================\n",
        "# txt_attn handling (only used for Flux.2-Klein-*):\n",
        "FLUX_TXT_ATTN_MODE = \"nvfp4\"        # \"nvfp4\" | \"bf16\" | \"fp8\"\n",
        "\n",
        "summary = convert_to_nvfp4(\n",
        "    input_path=input_path,\n",
        "    output_path=output_path,\n",
        "    model_type=MODEL_TYPE,\n",
        "    device=DEVICE,\n",
        "    match_official=MATCH_OFFICIAL,\n",
        "\n",
        "    # Flux Klein only:\n",
        "    txt_attn_mode=FLUX_TXT_ATTN_MODE,\n",
        "\n",
        "    # FP8 (used anywhere we choose FP8):\n",
        "    fp8_format=FP8_FORMAT,\n",
        "    fp8_scale_strategy=FP8_SCALE_STRATEGY,\n",
        "    fp8_slack=FP8_SLACK,\n",
        "    fp8_percentile=FP8_PERCENTILE,\n",
        "\n",
        "    # New behaviors:\n",
        "    blacklisted_2d_mode=(ZIMAGE_BLACKLISTED_2D_MODE if MODEL_TYPE.startswith(\"Z-Image\") else \"bf16\"),\n",
        "    nvfp4_fail_fallback=NVFP4_FAIL_FALLBACK,\n",
        "\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "print(\"‚úÖ Wrote:\", output_path)\n",
        "summary"
      ],
      "id": "kopQcgwViQyi"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YbA917ZeiQyj"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title üîê Hugging Face login (token)\n",
        "from getpass import getpass\n",
        "HF_TOKEN = getpass(\"Paste your Hugging Face token (write access):\").strip()\n",
        "assert HF_TOKEN, \"Token is required\"\n",
        "print(\"Token received (not printing it).\")\n"
      ],
      "id": "YbA917ZeiQyj"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6kpzRwC4iQyj"
      },
      "execution_count": null,
      "outputs": [],
      "source": [
        "#@title üèóÔ∏è Create repo 'quanttesting' and upload NVFP4 model\n",
        "import os\n",
        "from huggingface_hub import HfApi\n",
        "\n",
        "api = HfApi(token=HF_TOKEN)\n",
        "who = api.whoami()\n",
        "username = who[\"name\"]\n",
        "\n",
        "repo_name = \"YOUR REPO HERE\" # Change\n",
        "repo_id = f\"{username}/{repo_name}\"\n",
        "\n",
        "api.create_repo(repo_id=repo_id, repo_type=\"model\", exist_ok=True)\n",
        "print(\"Repo:\", repo_id)\n",
        "\n",
        "# Upload the NVFP4 safetensors (uses LFS automatically for large files)\n",
        "api.upload_file(\n",
        "    path_or_fileobj=output_path,\n",
        "    path_in_repo=os.path.basename(output_path),\n",
        "    repo_id=repo_id,\n",
        "    repo_type=\"model\",\n",
        "    commit_message=\"Upload NVFP4-converted\", # can change\n",
        ")\n",
        "print(\"‚úÖ Upload complete.\")\n"
      ],
      "id": "6kpzRwC4iQyj"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
